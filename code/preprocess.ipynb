{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xusj/miniconda3/envs/torch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle as pkl\n",
    "import jsonlines as jsonl\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline, EsmModel, EsmConfig, EsmTokenizer\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# os.environ['ALL_PROXY'] = 'socks5://127.0.0.1:10808'\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_ISOLATED_NODES = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Until No Isolated Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'DDI': pd.read_json(\"../json/drugbank_merged.json\"),\n",
    "    'DTI': pd.read_json(\"../json/dti_drugbank_final.json\"),\n",
    "    'PPI': pd.read_json(\"../json/ppi_primekg_final.json\"),\n",
    "    'Proteins': None,\n",
    "    'ProteinFeatures': None,\n",
    "    'Drugs': None,\n",
    "    'DrugFeatures': None,\n",
    "}\n",
    "\n",
    "has_isolated_nodes = IGNORE_ISOLATED_NODES\n",
    "while has_isolated_nodes:\n",
    "\n",
    "    DrugIDs = set(data['DDI']['Drug1_ID']) | set(data['DDI']['Drug2_ID'])\n",
    "    ProteinIDs = set(data['PPI']['Protein1_ID']) | set(data['PPI']['Protein2_ID'])\n",
    "    data['DTI'] = data['DTI'][data['DTI']['Drug_ID'].apply(DrugIDs.__contains__)\n",
    "                            & data['DTI']['Protein_ID'].apply(ProteinIDs.__contains__)]\n",
    "    \n",
    "    # print(len(DrugIDs), len(ProteinIDs))\n",
    "\n",
    "    has_isolated_nodes = False\n",
    "    DrugIDs_filtered = set(data['DTI']['Drug_ID'])\n",
    "    if DrugIDs_filtered != DrugIDs:\n",
    "        DrugIDs = DrugIDs_filtered\n",
    "        has_isolated_nodes = True\n",
    "        data['DDI'] = data['DDI'][data['DDI']['Drug1_ID'].apply(DrugIDs.__contains__)\n",
    "                                & data['DDI']['Drug2_ID'].apply(DrugIDs.__contains__)]\n",
    "    ProteinIDs_filtered = set(data['DTI']['Protein_ID'])\n",
    "    if ProteinIDs_filtered != ProteinIDs:\n",
    "        ProteinIDs = ProteinIDs_filtered\n",
    "        has_isolated_nodes = True\n",
    "        data['PPI'] = data['PPI'][data['PPI']['Protein1_ID'].apply(ProteinIDs.__contains__)\n",
    "                                & data['PPI']['Protein2_ID'].apply(ProteinIDs.__contains__)]\n",
    "\n",
    "data['Drugs'] = pd.DataFrame(\n",
    "    set(map(tuple, data['DDI'][[f'Drug1_ID', 'Drug1']].to_numpy())) | \\\n",
    "    set(map(tuple, data['DDI'][[f'Drug2_ID', 'Drug2']].to_numpy())),\n",
    "    columns=['Drug_ID', 'Drug']\n",
    ")\n",
    "data['Drugs'] = pd.merge(data['Drugs'], data['DTI'][['Drug_ID', 'Drug_name']].drop_duplicates(), on='Drug_ID').reset_index(drop=True)\n",
    "data['Proteins'] = pd.DataFrame(next(iter(jsonl.open(\"../json/names_to_protein_seq.json\"))).items(), columns=['Protein_name', 'Protein_seq'])\n",
    "data['Proteins'] = pd.merge(data['Proteins'], data['DTI'][['Protein_ID', 'Protein_name']].drop_duplicates().reset_index(drop=True), on='Protein_name')\n",
    "\n",
    "data['DDI'].reset_index(drop=True, inplace=True)\n",
    "data['DTI'].reset_index(drop=True, inplace=True)\n",
    "data['PPI'].reset_index(drop=True, inplace=True)\n",
    "\n",
    "data['DDI'].drop(['Drug1', 'Drug2'], axis=1, inplace=True)\n",
    "data['DTI'].drop(['Drug_name', 'Protein_name'], axis=1, inplace=True)\n",
    "data['PPI'].drop(['Protein1_name', 'Protein2_name'], axis=1, inplace=True)\n",
    "\n",
    "# data['Drugs'].reset_index(inplace=True)\n",
    "# data['Proteins'].reset_index(inplace=True)\n",
    "\n",
    "data['Drugs'].set_index('Drug_ID', inplace=True)\n",
    "data['Proteins'].set_index('Protein_ID', inplace=True)\n",
    "# data['DDI'].set_index(['Drug1_ID', 'Drug2_ID'], inplace=True)\n",
    "# data['DTI'].set_index(['Drug_ID', 'Protein_ID'], inplace=True)\n",
    "# data['PPI'].set_index(['Protein1_ID', 'Protein2_ID'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['PPI'] = data['PPI'][data['PPI']['Protein1_ID'] < data['PPI']['Protein2_ID']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepChem/ChemBERTa-77M-MLM were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'ESMTokenizer'. \n",
      "The class this function is called from is 'EsmTokenizer'.\n",
      "Some weights of the model checkpoint at facebook/esm-1b were not used when initializing EsmModel: ['esm.embeddings.token_type_embeddings.weight', 'lm_head.dense.weight', 'esm.embeddings.LayerNorm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'esm.embeddings.LayerNorm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing EsmModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm-1b and are newly initialized: ['esm.contact_head.regression.weight', 'esm.pooler.dense.bias', 'esm.pooler.dense.weight', 'esm.contact_head.regression.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "ChemBert = {\n",
    "    'tokenizer': AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\"),\n",
    "    'model': AutoModel.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\"),\n",
    "} \n",
    "\n",
    "Esm1b = {\n",
    "    'tokenizer': EsmTokenizer.from_pretrained('facebook/esm-1b', do_lower_case=False),\n",
    "    'model': AutoModel.from_pretrained(\"facebook/esm-1b\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1489 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1489/1489 [03:20<00:00,  7.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.01656273, -0.13354486,  0.08170871, ...,  0.01564058,\n",
       "        -0.15825166, -0.06826712],\n",
       "       [ 0.01084546,  0.04423308,  0.09004656, ..., -0.04558724,\n",
       "         0.05865683, -0.02220888],\n",
       "       [ 0.03101545,  0.05250555,  0.24257721, ..., -0.17679754,\n",
       "        -0.03301707,  0.14820105],\n",
       "       ...,\n",
       "       [-0.03918387,  0.08876041,  0.01355437, ..., -0.14501177,\n",
       "         0.09501589, -0.00566745],\n",
       "       [ 0.0034684 ,  0.21049589,  0.14795871, ...,  0.04313214,\n",
       "        -0.15063174,  0.05977998],\n",
       "       [-0.04802042,  0.22344407,  0.00190181, ..., -0.07860728,\n",
       "        -0.06015364,  0.06929832]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['DrugFeatures'] = np.array([ChemBert['model'](**ChemBert['tokenizer'](row['Drug'], return_tensors='pt', max_length=512, padding=True, truncation=True)).pooler_output[0].detach().numpy() for i, row in tqdm(data['Drugs'].iterrows(), total=len(data['Drugs']))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1310/1310 [1:17:50<00:00,  3.57s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.26701295,  0.10835341, -0.29234228, ..., -0.47279736,\n",
       "        -0.01228008, -0.32236168],\n",
       "       [-0.24903491,  0.1842002 , -0.20879096, ..., -0.45299304,\n",
       "         0.09167708, -0.13434085],\n",
       "       [-0.24209622,  0.12196819, -0.21449672, ..., -0.47724256,\n",
       "         0.05699752, -0.31131476],\n",
       "       ...,\n",
       "       [-0.29868805,  0.23641792, -0.2870817 , ..., -0.43996018,\n",
       "        -0.00714248, -0.2685453 ],\n",
       "       [-0.22394948,  0.11866497, -0.17858407, ..., -0.481605  ,\n",
       "         0.04997914, -0.24133028],\n",
       "       [-0.19244425, -0.05693382, -0.33932334, ..., -0.4666999 ,\n",
       "        -0.11490353, -0.40552577]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['ProteinFeatures'] = np.array([Esm1b['model'](**Esm1b['tokenizer'](row['Protein_seq'], return_tensors='pt', max_length=512, padding=True, truncation=True)).pooler_output[0].detach().numpy() for i, row in tqdm(data['Proteins'].iterrows(), total=len(data['Proteins']))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import DataStructs\n",
    "\n",
    "def calculate_similarity(smiles1, smiles2):\n",
    "    mol1 = Chem.MolFromSmiles(smiles1)\n",
    "    mol2 = Chem.MolFromSmiles(smiles2)\n",
    "\n",
    "    try:\n",
    "        fp1 = AllChem.GetMorganFingerprintAsBitVect(mol1, 2, nBits=2048)\n",
    "        fp2 = AllChem.GetMorganFingerprintAsBitVect(mol2, 2, nBits=2048)\n",
    "    except:\n",
    "        print(smiles1, smiles2)\n",
    "        return np.nan\n",
    "    similarity = DataStructs.TanimotoSimilarity(fp1, fp2)\n",
    "    return similarity\n",
    "\n",
    "data['DDI']['Similarity'] = data['DDI'].apply(lambda row: calculate_similarity(\n",
    "    data['Drugs']['Drug'][row['Drug1_ID']], \n",
    "    data['Drugs']['Drug'][row['Drug2_ID']]\n",
    "), axis=1)\n",
    "\n",
    "data['DDI'].dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(data, open('../dataset_filtered.pkl' if IGNORE_ISOLATED_NODES else '../dataset.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pkl.load(open('../dataset_filtered.pkl' if IGNORE_ISOLATED_NODES else '../dataset.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPI data is undirected\n"
     ]
    }
   ],
   "source": [
    "ppi_df = data['PPI'].set_index(['Protein1_ID', 'Protein2_ID'])\n",
    "try:\n",
    "    for p1, p2 in ppi_df.index:\n",
    "        assert (p2, p1) in ppi_df.index, f'{p1}, {p2} not in ppi_df'\n",
    "except AssertionError as e:\n",
    "    print(e)\n",
    "else:\n",
    "    print('PPI data is undirected')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
